关于用DQN写maze方法的日记记录

tensorflow的版本：
>>> tf.__version__
'2.9.1'


接下来要开始写A2C A3C的代码

SARSA：这是SARSA的部分，可以用来学习，唯一和Q-Learning不同的就是更新的时候是不一样的
https://blog.csdn.net/wydxry/article/details/119672202
SARSA写完了
------------------------------
接下来要用REINFORCE；蒙特卡洛策略梯度
theta是特略参数，需要找到一个策略的评价指标，然后根据随机梯度上升法来更新策略参数，最终不断最大化评价指标
需要找到采样的方式，能够使得通过采样样本估计的梯度的期望正比与真实的梯度，

策略梯度：https://www.sohu.com/a/303185520_100118081,感谢这个教程
策略是给定状态s时，动作集上的一个分布，直接对最优策略进行参数化建模，策略最终会成为一个概率密度函数，p(a|s,theta)

考虑一些模型的输出，
智能体每执行完一轮episode，就会形成一个轨迹，
我们最终的目的是使得，每个动作的乘积尽可能的大，
然后转化成对数似然函数，在乘上相应的权重，
[[0, 1], [0, 4], [14, 1], [14, 2],[1, 13], [1, 8], [2, 8], [2, 3], [3, 14], [3, 3], [4, 8], [4, 7], [5, 13], [5, 10], [6, 14], [6, 4], [7, 7], [7, 9], [8, 7], [8, 4], [9, 5], [9, 3], [10, 7], [10, 11], [11, 5], [11, 2], [12, 7], [12, 4], [13, 9], [13, 14]]

因为出现了循环所以我选择改变策略，撞墙就是-100，消耗是-1，奖励是100

